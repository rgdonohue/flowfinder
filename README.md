# FLOWFINDER: Watershed Delineation Research & Benchmark Framework

A research project exploring watershed delineation accuracy and developing systematic comparison methods for hydrological analysis tools. This work addresses key challenges in watershed delineation: reliability validation, systematic benchmarking, and geographic specialization for complex terrain.

![flow finder](images/flowfinder.png)

## ğŸ¯ Research Questions

**What problems are we trying to solve?**

1. **Reliability Gap**: How can we systematically validate watershed delineation tools across diverse terrain types?
2. **Benchmarking Gap**: Why is there no standardized framework for comparing watershed delineation tools?
3. **Geographic Bias**: How do existing tools perform in Mountain West terrain compared to other regions?
4. **Reproducibility Crisis**: How can we ensure watershed delineation results are reproducible and comparable?

**Our approach**: Develop FLOWFINDER as both a research tool and benchmark framework to systematically investigate these questions.

## ğŸ”¬ Research Context

### Current State of Watershed Delineation
- **Tool proliferation**: Multiple tools (TauDEM, GRASS, WhiteboxTools) with different algorithms
- **Validation challenges**: Limited systematic comparison of accuracy and performance
- **Geographic bias**: Most studies focus on eastern US or international basins
- **Reproducibility issues**: Ad-hoc validation methods make results hard to compare

### Research Gaps We're Addressing
- **Systematic benchmarking**: No standardized framework for multi-tool comparison
- **Mountain West terrain**: Limited research on complex terrain performance
- **Reliability metrics**: Need for consistent validation across tools
- **Open methodology**: Reproducible research practices for watershed analysis

## ğŸ“‹ Prerequisites

- Python 3.8+
- FLOWFINDER CLI tool installed and accessible
- Access to USGS NHD+ HR data and 3DEP 10m DEM data
- 8GB+ RAM recommended for processing large datasets
- Docker (for TauDEM integration)
- GRASS GIS (for r.watershed comparison)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd flowfinder

# Install dependencies
pip install -r requirements.txt

# Install FLOWFINDER
pip install flowfinder

# Copy environment template
cp .env.example .env
# Edit .env with your data paths and configuration
```

### 2. Configuration Setup

The system uses a hierarchical configuration architecture to manage complexity:

```bash
# Create configuration structure
mkdir -p config/{base,environments,tools,experiments,schemas}

# Environment-specific configurations
config/environments/development.yaml    # Local dev (10 basins)
config/environments/testing.yaml        # CI/testing (50 basins)  
config/environments/production.yaml     # Full-scale (500+ basins)

# Tool-specific configurations
config/tools/flowfinder.yaml            # FLOWFINDER settings
config/tools/taudem.yaml               # TauDEM MPI settings
config/tools/grass.yaml                # GRASS r.watershed settings
config/tools/whitebox.yaml             # WhiteboxTools settings
```

### 3. Data Preparation

Place your input datasets in the `data/` directory:

```
data/
â”œâ”€â”€ huc12_mountain_west.shp    # HUC12 boundaries for Mountain West
â”œâ”€â”€ nhd_hr_catchments.shp      # NHD+ HR catchment polygons
â”œâ”€â”€ nhd_flowlines.shp          # NHD+ HR flowlines
â””â”€â”€ dem_10m.tif               # 10m DEM mosaic or tiles
```

### 4. Run Single-Tool Benchmark

```bash
# Step 1: Generate stratified basin sample
python scripts/basin_sampler.py --config config/basin_sampler_config.yaml

# Step 2: Extract truth polygons
python scripts/truth_extractor.py --config config/truth_extractor_config.yaml

# Step 3: Run FLOWFINDER benchmark
python scripts/benchmark_runner.py \
    --sample basin_sample.csv \
    --truth truth_polygons.gpkg \
    --config config/benchmark_config.yaml \
    --outdir results/
```

### 5. Run Multi-Tool Comparison (Experimental)

```bash
# Using the new configuration system
python scripts/benchmark_runner_integrated.py \
    --environment development \
    --tools flowfinder,taudem \
    --experiment accuracy_comparison \
    --outdir results/multi_tool/
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md                    # Project overview + setup
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ pyproject.toml              # Modern Python project config
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ .gitignore                  # Standard Python gitignore
â”‚
â”œâ”€â”€ config/                     # Hierarchical configuration system
â”‚   â”œâ”€â”€ base.yaml              # Foundation configurations
â”‚   â”œâ”€â”€ configuration_manager.py # Configuration inheritance system
â”‚   â”œâ”€â”€ environments/           # Environment-specific settings
â”‚   â”‚   â”œâ”€â”€ development.yaml   # Local development (10 basins)
â”‚   â”‚   â”œâ”€â”€ testing.yaml       # CI/testing (50 basins)
â”‚   â”‚   â””â”€â”€ production.yaml    # Full-scale (500+ basins)
â”‚   â”œâ”€â”€ tools/                  # Tool-specific configurations
â”‚   â”‚   â”œâ”€â”€ flowfinder.yaml    # FLOWFINDER settings
â”‚   â”‚   â”œâ”€â”€ taudem.yaml        # TauDEM MPI settings
â”‚   â”‚   â”œâ”€â”€ grass.yaml         # GRASS r.watershed settings
â”‚   â”‚   â””â”€â”€ whitebox.yaml      # WhiteboxTools settings
â”‚   â””â”€â”€ schemas/               # JSON Schema validation
â”‚
â”œâ”€â”€ scripts/                    # Core benchmark scripts
â”‚   â”œâ”€â”€ basin_sampler.py       # Stratified basin sampling
â”‚   â”œâ”€â”€ truth_extractor.py     # Truth polygon extraction
â”‚   â”œâ”€â”€ benchmark_runner.py    # FLOWFINDER accuracy testing
â”‚   â”œâ”€â”€ benchmark_runner_integrated.py # Multi-tool comparison
â”‚   â””â”€â”€ tool_adapters/         # Tool adapter implementations
â”‚
â”œâ”€â”€ data/                       # Input datasets (gitignored)
â”œâ”€â”€ results/                    # Output directory (gitignored)
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ docs/                       # Research and technical documentation
â”‚   â”œâ”€â”€ strategic_analysis_implementation_roadmap_v2.md # Research roadmap
â”‚   â”œâ”€â”€ multi_tool_integration_strategy.md # Integration approach
â”‚   â”œâ”€â”€ strategic_analysis_assessment.md # Research evaluation
â”‚   â”œâ”€â”€ immediate_next_steps.md # Implementation priorities
â”‚   â”œâ”€â”€ configuration_architecture.md # Configuration system design
â”‚   â”œâ”€â”€ multi_tool_benchmark_architecture.md # Framework design
â”‚   â””â”€â”€ test_coverage/          # Test coverage documentation
â”‚
â””â”€â”€ notebooks/                  # Jupyter exploration
    â””â”€â”€ benchmark_analysis.ipynb
```

## ğŸ”§ Configuration Architecture

The system uses a **hierarchical configuration architecture** to manage complexity across different tools and environments:

### Configuration Hierarchy
```
Base Configurations â†’ Environment â†’ Tool â†’ Local Overrides
```

### Example Configuration Composition
```yaml
# Development FLOWFINDER experiment
inherits:
  - "base/regions.yaml#mountain_west_minimal"
  - "base/quality_standards.yaml#development_grade"
  - "environments/development.yaml"
  - "tools/flowfinder/base.yaml"
  - "experiments/accuracy_comparison.yaml"

overrides:
  basin_sampling:
    n_per_stratum: 1  # Minimal for dev
  benchmark:
    timeout_seconds: 30  # Quick timeout
```

### Tool Adapter Interface
```python
class ToolAdapter(ABC):
    @abstractmethod
    def delineate_watershed(self, pour_point: Point, dem_path: str) -> Tuple[Polygon, Dict]:
        """Delineate watershed and return polygon + performance metrics"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if tool is available on system"""
        pass
```

## ğŸ“Š Research Outputs

### Single-Tool Benchmark
- `benchmark_results.json`: Detailed per-basin metrics
- `accuracy_summary.csv`: Tabular results for analysis
- `benchmark_summary.txt`: Performance analysis and key findings

### Multi-Tool Comparison (Experimental)
- `multi_tool_results.json`: Comparative analysis across tools
- `performance_comparison.csv`: Runtime and memory comparisons
- `statistical_analysis.csv`: ANOVA, Tukey HSD, Kruskal-Wallis results
- `publication_figures/`: Research-ready charts and graphs

## ğŸ¯ Research Metrics

### Technical Validation
| Metric                    | Current Target                    | Status |
| ------------------------- | --------------------------------- | ------ |
| FLOWFINDER IOU (mean)     | â‰¥ 0.90                           | ğŸ”„ In Progress |
| FLOWFINDER IOU (90th percentile) | â‰¥ 0.95                       | ğŸ”„ In Progress |
| Runtime (mean)            | â‰¤ 30 s                           | ğŸ”„ In Progress |
| Configuration redundancy  | 90% reduction                    | âœ… Achieved |
| Tool integration success  | 4 major tools integrated         | ğŸ”„ In Progress |

### Research Impact Goals
| Metric                    | Target                           | Status |
| ------------------------- | --------------------------------- | ------ |
| Peer-reviewed publications | 2+ papers submitted              | ğŸ”„ In Progress |
| Conference presentations  | 5+ presentations                 | ğŸ”„ In Progress |
| Citations (2 years)       | 100+ citations                   | ğŸ”„ In Progress |
| Framework adoption        | 3+ external research groups      | ğŸ”„ In Progress |

### Community Engagement Goals
| Metric                    | Target                           | Status |
| ------------------------- | --------------------------------- | ------ |
| GitHub stars              | 500+ stars                       | ğŸ”„ In Progress |
| FLOWFINDER downloads      | 1000+ downloads                  | ğŸ”„ In Progress |
| External contributors     | 10+ contributors                 | ğŸ”„ In Progress |
| Institutional adoptions   | 5+ adoptions                     | ğŸ”„ In Progress |

## ğŸ§ª Testing

```bash
# Run unit tests
python -m pytest tests/

# Test configuration system
python test_configuration_system.py

# Test multi-tool integration
python test_integration.py

# Run with coverage
python -m pytest tests/ --cov=scripts --cov-report=html
```

## ğŸ“ˆ Analysis

Use the Jupyter notebook for detailed analysis:

```bash
# Start Jupyter
jupyter lab notebooks/

# Open benchmark_analysis.ipynb for interactive exploration
```

## ğŸ¯ Research Roadmap

### Phase 1: Foundation - IN PROGRESS
- âœ… **Configuration Architecture**: Hierarchical system implemented
- âœ… **FLOWFINDER Development**: Core tool with validation framework
- ğŸ”„ **Benchmark Framework MVP**: Multi-tool comparison development
- ğŸ”„ **Literature Review**: Research gap analysis and methodology development

### Phase 2: Tool Integration - PLANNED
- ğŸ”„ **WhiteboxTools Integration**: Rust-based performance comparison
- ğŸ”„ **TauDEM Integration**: Academic gold standard validation
- ğŸ”„ **GRASS GIS Integration**: Comprehensive hydrological suite
- ğŸ”„ **SAGA GIS Integration**: European academic adoption

## ğŸ“š Documentation

### Research Documents
- **[Research Roadmap](docs/strategic_analysis_implementation_roadmap_v2.md)**: Implementation plan with research milestones
- **[Multi-Tool Integration Strategy](docs/multi_tool_integration_strategy.md)**: Research-based tool integration approach
- **[Research Assessment](docs/strategic_analysis_assessment.md)**: Comprehensive research evaluation
- **[Next Steps](docs/immediate_next_steps.md)**: Implementation priorities

### Technical Documents
- **[Configuration Architecture](docs/configuration_architecture.md)**: Hierarchical configuration system design
- **[Multi-Tool Benchmark Architecture](docs/multi_tool_benchmark_architecture.md)**: Framework design and implementation
- **[Test Coverage](docs/test_coverage/)**: Comprehensive testing documentation

## ğŸ¤ Contributing

We welcome contributions from the research community:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/research-improvement`)
3. Commit your changes (`git commit -m 'Add research improvement'`)
4. Push to the branch (`git push origin feature/research-improvement`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- USGS for NHD+ HR and 3DEP data
- FLOWFINDER development team
- Open source geospatial community
- Academic research community for feedback and validation

## ğŸ“ Support

For research questions and technical issues:
- Check the [documentation](docs/)
- Review the [Research Roadmap](docs/strategic_analysis_implementation_roadmap_v2.md)
- See the [Multi-Tool Integration Strategy](docs/multi_tool_integration_strategy.md)
- Open an issue on GitHub

---

*"Research is formalized curiosity. It is poking and prying with a purpose."*

**FLOWFINDER: Exploring watershed delineation accuracy and developing systematic comparison methods for hydrological research.** 